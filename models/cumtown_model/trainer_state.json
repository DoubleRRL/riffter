{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.22844958879074018,
  "eval_steps": 500,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0015229972586049345,
      "grad_norm": 174.7003173828125,
      "learning_rate": 1.8000000000000001e-06,
      "loss": 9.6354,
      "step": 10
    },
    {
      "epoch": 0.003045994517209869,
      "grad_norm": 104.70088195800781,
      "learning_rate": 3.8000000000000005e-06,
      "loss": 8.9939,
      "step": 20
    },
    {
      "epoch": 0.004568991775814804,
      "grad_norm": 63.31829833984375,
      "learning_rate": 5.8e-06,
      "loss": 7.9298,
      "step": 30
    },
    {
      "epoch": 0.006091989034419738,
      "grad_norm": 38.871604919433594,
      "learning_rate": 7.800000000000002e-06,
      "loss": 7.1397,
      "step": 40
    },
    {
      "epoch": 0.007614986293024673,
      "grad_norm": 36.42048645019531,
      "learning_rate": 9.800000000000001e-06,
      "loss": 6.4019,
      "step": 50
    },
    {
      "epoch": 0.009137983551629607,
      "grad_norm": 21.133758544921875,
      "learning_rate": 1.18e-05,
      "loss": 5.8808,
      "step": 60
    },
    {
      "epoch": 0.010660980810234541,
      "grad_norm": 13.963455200195312,
      "learning_rate": 1.38e-05,
      "loss": 5.5506,
      "step": 70
    },
    {
      "epoch": 0.012183978068839476,
      "grad_norm": 11.226394653320312,
      "learning_rate": 1.58e-05,
      "loss": 5.232,
      "step": 80
    },
    {
      "epoch": 0.01370697532744441,
      "grad_norm": 8.552190780639648,
      "learning_rate": 1.7800000000000002e-05,
      "loss": 5.1731,
      "step": 90
    },
    {
      "epoch": 0.015229972586049346,
      "grad_norm": 6.378309726715088,
      "learning_rate": 1.98e-05,
      "loss": 4.9217,
      "step": 100
    },
    {
      "epoch": 0.01675296984465428,
      "grad_norm": 6.1062211990356445,
      "learning_rate": 1.9990815389325443e-05,
      "loss": 4.8498,
      "step": 110
    },
    {
      "epoch": 0.018275967103259214,
      "grad_norm": 5.9528679847717285,
      "learning_rate": 1.9980610266353712e-05,
      "loss": 4.7612,
      "step": 120
    },
    {
      "epoch": 0.01979896436186415,
      "grad_norm": 5.533873081207275,
      "learning_rate": 1.9970405143381978e-05,
      "loss": 4.7652,
      "step": 130
    },
    {
      "epoch": 0.021321961620469083,
      "grad_norm": 5.740993022918701,
      "learning_rate": 1.9960200020410247e-05,
      "loss": 4.6701,
      "step": 140
    },
    {
      "epoch": 0.022844958879074017,
      "grad_norm": 4.639575481414795,
      "learning_rate": 1.9949994897438517e-05,
      "loss": 4.563,
      "step": 150
    },
    {
      "epoch": 0.02436795613767895,
      "grad_norm": 5.190456867218018,
      "learning_rate": 1.9939789774466786e-05,
      "loss": 4.6121,
      "step": 160
    },
    {
      "epoch": 0.025890953396283885,
      "grad_norm": 5.123432636260986,
      "learning_rate": 1.9929584651495052e-05,
      "loss": 4.6662,
      "step": 170
    },
    {
      "epoch": 0.02741395065488882,
      "grad_norm": 5.122714042663574,
      "learning_rate": 1.9919379528523318e-05,
      "loss": 4.4974,
      "step": 180
    },
    {
      "epoch": 0.028936947913493757,
      "grad_norm": 6.769731521606445,
      "learning_rate": 1.9909174405551587e-05,
      "loss": 4.4824,
      "step": 190
    },
    {
      "epoch": 0.03045994517209869,
      "grad_norm": 4.535389423370361,
      "learning_rate": 1.9898969282579856e-05,
      "loss": 4.4107,
      "step": 200
    },
    {
      "epoch": 0.031982942430703626,
      "grad_norm": 5.431365489959717,
      "learning_rate": 1.9888764159608126e-05,
      "loss": 4.539,
      "step": 210
    },
    {
      "epoch": 0.03350593968930856,
      "grad_norm": 4.198873519897461,
      "learning_rate": 1.9878559036636395e-05,
      "loss": 4.3959,
      "step": 220
    },
    {
      "epoch": 0.035028936947913494,
      "grad_norm": 5.16107702255249,
      "learning_rate": 1.986835391366466e-05,
      "loss": 4.3857,
      "step": 230
    },
    {
      "epoch": 0.03655193420651843,
      "grad_norm": 4.3036017417907715,
      "learning_rate": 1.985814879069293e-05,
      "loss": 4.4308,
      "step": 240
    },
    {
      "epoch": 0.03807493146512336,
      "grad_norm": 4.36737585067749,
      "learning_rate": 1.9847943667721196e-05,
      "loss": 4.3136,
      "step": 250
    },
    {
      "epoch": 0.0395979287237283,
      "grad_norm": 4.125537395477295,
      "learning_rate": 1.9837738544749466e-05,
      "loss": 4.2908,
      "step": 260
    },
    {
      "epoch": 0.04112092598233323,
      "grad_norm": 4.230688095092773,
      "learning_rate": 1.9827533421777735e-05,
      "loss": 4.2237,
      "step": 270
    },
    {
      "epoch": 0.042643923240938165,
      "grad_norm": 3.877727508544922,
      "learning_rate": 1.9817328298806e-05,
      "loss": 4.2307,
      "step": 280
    },
    {
      "epoch": 0.0441669204995431,
      "grad_norm": 4.060988426208496,
      "learning_rate": 1.980712317583427e-05,
      "loss": 4.2121,
      "step": 290
    },
    {
      "epoch": 0.045689917758148034,
      "grad_norm": 4.933369159698486,
      "learning_rate": 1.979691805286254e-05,
      "loss": 4.1894,
      "step": 300
    },
    {
      "epoch": 0.04721291501675297,
      "grad_norm": 3.3966064453125,
      "learning_rate": 1.978671292989081e-05,
      "loss": 4.3437,
      "step": 310
    },
    {
      "epoch": 0.0487359122753579,
      "grad_norm": 3.964554786682129,
      "learning_rate": 1.9776507806919075e-05,
      "loss": 4.2522,
      "step": 320
    },
    {
      "epoch": 0.05025890953396284,
      "grad_norm": 4.153674125671387,
      "learning_rate": 1.9766302683947344e-05,
      "loss": 4.247,
      "step": 330
    },
    {
      "epoch": 0.05178190679256777,
      "grad_norm": 3.6069486141204834,
      "learning_rate": 1.975609756097561e-05,
      "loss": 4.2177,
      "step": 340
    },
    {
      "epoch": 0.053304904051172705,
      "grad_norm": 3.9388487339019775,
      "learning_rate": 1.974589243800388e-05,
      "loss": 4.1157,
      "step": 350
    },
    {
      "epoch": 0.05482790130977764,
      "grad_norm": 3.9671521186828613,
      "learning_rate": 1.973568731503215e-05,
      "loss": 4.2398,
      "step": 360
    },
    {
      "epoch": 0.056350898568382574,
      "grad_norm": 3.5097031593322754,
      "learning_rate": 1.9725482192060418e-05,
      "loss": 4.0742,
      "step": 370
    },
    {
      "epoch": 0.057873895826987515,
      "grad_norm": 3.8518178462982178,
      "learning_rate": 1.9715277069088684e-05,
      "loss": 4.0972,
      "step": 380
    },
    {
      "epoch": 0.05939689308559245,
      "grad_norm": 3.145073890686035,
      "learning_rate": 1.970507194611695e-05,
      "loss": 4.1905,
      "step": 390
    },
    {
      "epoch": 0.06091989034419738,
      "grad_norm": 4.225291728973389,
      "learning_rate": 1.969486682314522e-05,
      "loss": 4.0769,
      "step": 400
    },
    {
      "epoch": 0.06244288760280232,
      "grad_norm": 3.763441324234009,
      "learning_rate": 1.968466170017349e-05,
      "loss": 4.0562,
      "step": 410
    },
    {
      "epoch": 0.06396588486140725,
      "grad_norm": 3.7430505752563477,
      "learning_rate": 1.9674456577201758e-05,
      "loss": 4.193,
      "step": 420
    },
    {
      "epoch": 0.06548888212001218,
      "grad_norm": 3.2744226455688477,
      "learning_rate": 1.9664251454230027e-05,
      "loss": 4.0866,
      "step": 430
    },
    {
      "epoch": 0.06701187937861712,
      "grad_norm": 3.36356258392334,
      "learning_rate": 1.9654046331258293e-05,
      "loss": 4.1152,
      "step": 440
    },
    {
      "epoch": 0.06853487663722205,
      "grad_norm": 3.5444443225860596,
      "learning_rate": 1.964384120828656e-05,
      "loss": 4.02,
      "step": 450
    },
    {
      "epoch": 0.07005787389582699,
      "grad_norm": 3.7664201259613037,
      "learning_rate": 1.963363608531483e-05,
      "loss": 4.0088,
      "step": 460
    },
    {
      "epoch": 0.07158087115443192,
      "grad_norm": 3.261906623840332,
      "learning_rate": 1.9623430962343098e-05,
      "loss": 4.0416,
      "step": 470
    },
    {
      "epoch": 0.07310386841303686,
      "grad_norm": 3.33663272857666,
      "learning_rate": 1.9613225839371367e-05,
      "loss": 4.1608,
      "step": 480
    },
    {
      "epoch": 0.07462686567164178,
      "grad_norm": 3.620957374572754,
      "learning_rate": 1.9603020716399633e-05,
      "loss": 4.1419,
      "step": 490
    },
    {
      "epoch": 0.07614986293024673,
      "grad_norm": 3.6007282733917236,
      "learning_rate": 1.9592815593427902e-05,
      "loss": 4.0576,
      "step": 500
    },
    {
      "epoch": 0.07767286018885167,
      "grad_norm": 3.8830113410949707,
      "learning_rate": 1.9582610470456172e-05,
      "loss": 4.0614,
      "step": 510
    },
    {
      "epoch": 0.0791958574474566,
      "grad_norm": 3.636482000350952,
      "learning_rate": 1.9572405347484438e-05,
      "loss": 4.1013,
      "step": 520
    },
    {
      "epoch": 0.08071885470606154,
      "grad_norm": 4.0475335121154785,
      "learning_rate": 1.9562200224512707e-05,
      "loss": 4.0604,
      "step": 530
    },
    {
      "epoch": 0.08224185196466646,
      "grad_norm": 3.9247758388519287,
      "learning_rate": 1.9551995101540976e-05,
      "loss": 3.9874,
      "step": 540
    },
    {
      "epoch": 0.0837648492232714,
      "grad_norm": 3.127474069595337,
      "learning_rate": 1.9541789978569242e-05,
      "loss": 3.9774,
      "step": 550
    },
    {
      "epoch": 0.08528784648187633,
      "grad_norm": 3.788961887359619,
      "learning_rate": 1.953158485559751e-05,
      "loss": 3.9386,
      "step": 560
    },
    {
      "epoch": 0.08681084374048127,
      "grad_norm": 3.253800630569458,
      "learning_rate": 1.952137973262578e-05,
      "loss": 4.0169,
      "step": 570
    },
    {
      "epoch": 0.0883338409990862,
      "grad_norm": 3.6370623111724854,
      "learning_rate": 1.951117460965405e-05,
      "loss": 3.978,
      "step": 580
    },
    {
      "epoch": 0.08985683825769114,
      "grad_norm": 3.065119743347168,
      "learning_rate": 1.9500969486682316e-05,
      "loss": 4.0265,
      "step": 590
    },
    {
      "epoch": 0.09137983551629607,
      "grad_norm": 3.107194185256958,
      "learning_rate": 1.9490764363710582e-05,
      "loss": 4.0257,
      "step": 600
    },
    {
      "epoch": 0.09290283277490101,
      "grad_norm": 3.7161858081817627,
      "learning_rate": 1.948055924073885e-05,
      "loss": 4.033,
      "step": 610
    },
    {
      "epoch": 0.09442583003350594,
      "grad_norm": 3.057060480117798,
      "learning_rate": 1.947035411776712e-05,
      "loss": 4.0758,
      "step": 620
    },
    {
      "epoch": 0.09594882729211088,
      "grad_norm": 3.295006275177002,
      "learning_rate": 1.946014899479539e-05,
      "loss": 3.9703,
      "step": 630
    },
    {
      "epoch": 0.0974718245507158,
      "grad_norm": 3.645376443862915,
      "learning_rate": 1.944994387182366e-05,
      "loss": 3.9767,
      "step": 640
    },
    {
      "epoch": 0.09899482180932075,
      "grad_norm": 2.944681406021118,
      "learning_rate": 1.9439738748851925e-05,
      "loss": 4.0735,
      "step": 650
    },
    {
      "epoch": 0.10051781906792567,
      "grad_norm": 2.7204551696777344,
      "learning_rate": 1.942953362588019e-05,
      "loss": 3.9638,
      "step": 660
    },
    {
      "epoch": 0.10204081632653061,
      "grad_norm": 3.1111083030700684,
      "learning_rate": 1.941932850290846e-05,
      "loss": 3.9622,
      "step": 670
    },
    {
      "epoch": 0.10356381358513554,
      "grad_norm": 2.939690589904785,
      "learning_rate": 1.940912337993673e-05,
      "loss": 3.9704,
      "step": 680
    },
    {
      "epoch": 0.10508681084374048,
      "grad_norm": 3.2781589031219482,
      "learning_rate": 1.9398918256965e-05,
      "loss": 4.0271,
      "step": 690
    },
    {
      "epoch": 0.10660980810234541,
      "grad_norm": 3.8668174743652344,
      "learning_rate": 1.9388713133993265e-05,
      "loss": 4.0392,
      "step": 700
    },
    {
      "epoch": 0.10813280536095035,
      "grad_norm": 3.219205856323242,
      "learning_rate": 1.9378508011021535e-05,
      "loss": 3.9552,
      "step": 710
    },
    {
      "epoch": 0.10965580261955528,
      "grad_norm": 2.9149203300476074,
      "learning_rate": 1.9368302888049804e-05,
      "loss": 3.9778,
      "step": 720
    },
    {
      "epoch": 0.11117879987816022,
      "grad_norm": 2.858269214630127,
      "learning_rate": 1.935809776507807e-05,
      "loss": 3.994,
      "step": 730
    },
    {
      "epoch": 0.11270179713676515,
      "grad_norm": 2.6826071739196777,
      "learning_rate": 1.934789264210634e-05,
      "loss": 3.9705,
      "step": 740
    },
    {
      "epoch": 0.11422479439537009,
      "grad_norm": 3.2810823917388916,
      "learning_rate": 1.933768751913461e-05,
      "loss": 4.0527,
      "step": 750
    },
    {
      "epoch": 0.11574779165397503,
      "grad_norm": 3.2899038791656494,
      "learning_rate": 1.9327482396162874e-05,
      "loss": 3.8528,
      "step": 760
    },
    {
      "epoch": 0.11727078891257996,
      "grad_norm": 2.8957977294921875,
      "learning_rate": 1.9317277273191144e-05,
      "loss": 4.0762,
      "step": 770
    },
    {
      "epoch": 0.1187937861711849,
      "grad_norm": 3.3655190467834473,
      "learning_rate": 1.9307072150219413e-05,
      "loss": 3.9097,
      "step": 780
    },
    {
      "epoch": 0.12031678342978983,
      "grad_norm": 2.730194568634033,
      "learning_rate": 1.9296867027247682e-05,
      "loss": 3.9472,
      "step": 790
    },
    {
      "epoch": 0.12183978068839477,
      "grad_norm": 2.8229727745056152,
      "learning_rate": 1.928666190427595e-05,
      "loss": 4.0324,
      "step": 800
    },
    {
      "epoch": 0.1233627779469997,
      "grad_norm": 3.156935930252075,
      "learning_rate": 1.9276456781304214e-05,
      "loss": 3.8506,
      "step": 810
    },
    {
      "epoch": 0.12488577520560463,
      "grad_norm": 3.176156759262085,
      "learning_rate": 1.9266251658332484e-05,
      "loss": 4.0519,
      "step": 820
    },
    {
      "epoch": 0.12640877246420956,
      "grad_norm": 3.0072078704833984,
      "learning_rate": 1.9256046535360753e-05,
      "loss": 3.9804,
      "step": 830
    },
    {
      "epoch": 0.1279317697228145,
      "grad_norm": 2.8039815425872803,
      "learning_rate": 1.9245841412389022e-05,
      "loss": 3.9813,
      "step": 840
    },
    {
      "epoch": 0.12945476698141944,
      "grad_norm": 3.259500026702881,
      "learning_rate": 1.923563628941729e-05,
      "loss": 3.9883,
      "step": 850
    },
    {
      "epoch": 0.13097776424002436,
      "grad_norm": 2.7899069786071777,
      "learning_rate": 1.9225431166445557e-05,
      "loss": 3.8859,
      "step": 860
    },
    {
      "epoch": 0.1325007614986293,
      "grad_norm": 3.1633853912353516,
      "learning_rate": 1.9215226043473823e-05,
      "loss": 3.7851,
      "step": 870
    },
    {
      "epoch": 0.13402375875723424,
      "grad_norm": 2.761103391647339,
      "learning_rate": 1.9205020920502093e-05,
      "loss": 3.9427,
      "step": 880
    },
    {
      "epoch": 0.13554675601583918,
      "grad_norm": 3.188291072845459,
      "learning_rate": 1.9194815797530362e-05,
      "loss": 3.8475,
      "step": 890
    },
    {
      "epoch": 0.1370697532744441,
      "grad_norm": 3.62170672416687,
      "learning_rate": 1.918461067455863e-05,
      "loss": 3.846,
      "step": 900
    },
    {
      "epoch": 0.13859275053304904,
      "grad_norm": 2.8310482501983643,
      "learning_rate": 1.9174405551586897e-05,
      "loss": 3.965,
      "step": 910
    },
    {
      "epoch": 0.14011574779165398,
      "grad_norm": 2.8986105918884277,
      "learning_rate": 1.9164200428615167e-05,
      "loss": 3.9538,
      "step": 920
    },
    {
      "epoch": 0.14163874505025892,
      "grad_norm": 2.5529890060424805,
      "learning_rate": 1.9153995305643433e-05,
      "loss": 3.9534,
      "step": 930
    },
    {
      "epoch": 0.14316174230886383,
      "grad_norm": 3.2001712322235107,
      "learning_rate": 1.9143790182671702e-05,
      "loss": 4.0421,
      "step": 940
    },
    {
      "epoch": 0.14468473956746877,
      "grad_norm": 3.1053714752197266,
      "learning_rate": 1.913358505969997e-05,
      "loss": 3.9009,
      "step": 950
    },
    {
      "epoch": 0.14620773682607371,
      "grad_norm": 2.952592611312866,
      "learning_rate": 1.912337993672824e-05,
      "loss": 3.9481,
      "step": 960
    },
    {
      "epoch": 0.14773073408467866,
      "grad_norm": 3.005601167678833,
      "learning_rate": 1.9113174813756507e-05,
      "loss": 3.9721,
      "step": 970
    },
    {
      "epoch": 0.14925373134328357,
      "grad_norm": 3.5025522708892822,
      "learning_rate": 1.9102969690784776e-05,
      "loss": 3.9607,
      "step": 980
    },
    {
      "epoch": 0.1507767286018885,
      "grad_norm": 2.7761192321777344,
      "learning_rate": 1.9092764567813045e-05,
      "loss": 3.8615,
      "step": 990
    },
    {
      "epoch": 0.15229972586049345,
      "grad_norm": 3.035818576812744,
      "learning_rate": 1.908255944484131e-05,
      "loss": 4.0644,
      "step": 1000
    },
    {
      "epoch": 0.1538227231190984,
      "grad_norm": 2.8450326919555664,
      "learning_rate": 1.907235432186958e-05,
      "loss": 3.8359,
      "step": 1010
    },
    {
      "epoch": 0.15534572037770333,
      "grad_norm": 2.6891684532165527,
      "learning_rate": 1.9062149198897846e-05,
      "loss": 3.9275,
      "step": 1020
    },
    {
      "epoch": 0.15686871763630825,
      "grad_norm": 2.754908800125122,
      "learning_rate": 1.9051944075926116e-05,
      "loss": 3.8934,
      "step": 1030
    },
    {
      "epoch": 0.1583917148949132,
      "grad_norm": 2.80637788772583,
      "learning_rate": 1.9041738952954385e-05,
      "loss": 3.9948,
      "step": 1040
    },
    {
      "epoch": 0.15991471215351813,
      "grad_norm": 3.1093556880950928,
      "learning_rate": 1.9031533829982654e-05,
      "loss": 3.9555,
      "step": 1050
    },
    {
      "epoch": 0.16143770941212307,
      "grad_norm": 3.077634572982788,
      "learning_rate": 1.9021328707010924e-05,
      "loss": 3.904,
      "step": 1060
    },
    {
      "epoch": 0.16296070667072798,
      "grad_norm": 2.740277051925659,
      "learning_rate": 1.901112358403919e-05,
      "loss": 3.8943,
      "step": 1070
    },
    {
      "epoch": 0.16448370392933293,
      "grad_norm": 2.913686513900757,
      "learning_rate": 1.9000918461067456e-05,
      "loss": 3.8675,
      "step": 1080
    },
    {
      "epoch": 0.16600670118793787,
      "grad_norm": 3.4098446369171143,
      "learning_rate": 1.8990713338095725e-05,
      "loss": 3.9064,
      "step": 1090
    },
    {
      "epoch": 0.1675296984465428,
      "grad_norm": 2.808295488357544,
      "learning_rate": 1.8980508215123994e-05,
      "loss": 3.918,
      "step": 1100
    },
    {
      "epoch": 0.16905269570514772,
      "grad_norm": 2.8754847049713135,
      "learning_rate": 1.8970303092152264e-05,
      "loss": 3.9105,
      "step": 1110
    },
    {
      "epoch": 0.17057569296375266,
      "grad_norm": 3.2201988697052,
      "learning_rate": 1.896009796918053e-05,
      "loss": 4.0039,
      "step": 1120
    },
    {
      "epoch": 0.1720986902223576,
      "grad_norm": 3.1210224628448486,
      "learning_rate": 1.89498928462088e-05,
      "loss": 3.8575,
      "step": 1130
    },
    {
      "epoch": 0.17362168748096254,
      "grad_norm": 2.955735206604004,
      "learning_rate": 1.8939687723237065e-05,
      "loss": 3.8495,
      "step": 1140
    },
    {
      "epoch": 0.17514468473956746,
      "grad_norm": 2.849792718887329,
      "learning_rate": 1.8929482600265334e-05,
      "loss": 3.9162,
      "step": 1150
    },
    {
      "epoch": 0.1766676819981724,
      "grad_norm": 2.750469446182251,
      "learning_rate": 1.8919277477293603e-05,
      "loss": 3.894,
      "step": 1160
    },
    {
      "epoch": 0.17819067925677734,
      "grad_norm": 3.029008150100708,
      "learning_rate": 1.8909072354321873e-05,
      "loss": 3.9426,
      "step": 1170
    },
    {
      "epoch": 0.17971367651538228,
      "grad_norm": 3.12742280960083,
      "learning_rate": 1.889886723135014e-05,
      "loss": 3.914,
      "step": 1180
    },
    {
      "epoch": 0.1812366737739872,
      "grad_norm": 2.845797061920166,
      "learning_rate": 1.8888662108378408e-05,
      "loss": 3.9365,
      "step": 1190
    },
    {
      "epoch": 0.18275967103259214,
      "grad_norm": 3.4472408294677734,
      "learning_rate": 1.8878456985406674e-05,
      "loss": 3.8582,
      "step": 1200
    },
    {
      "epoch": 0.18428266829119708,
      "grad_norm": 3.084925413131714,
      "learning_rate": 1.8868251862434943e-05,
      "loss": 3.9151,
      "step": 1210
    },
    {
      "epoch": 0.18580566554980202,
      "grad_norm": 2.786257743835449,
      "learning_rate": 1.8858046739463213e-05,
      "loss": 3.8872,
      "step": 1220
    },
    {
      "epoch": 0.18732866280840693,
      "grad_norm": 2.975944995880127,
      "learning_rate": 1.884784161649148e-05,
      "loss": 3.9095,
      "step": 1230
    },
    {
      "epoch": 0.18885166006701187,
      "grad_norm": 2.71213436126709,
      "learning_rate": 1.8837636493519748e-05,
      "loss": 3.8579,
      "step": 1240
    },
    {
      "epoch": 0.19037465732561681,
      "grad_norm": 3.079315185546875,
      "learning_rate": 1.8827431370548017e-05,
      "loss": 3.9151,
      "step": 1250
    },
    {
      "epoch": 0.19189765458422176,
      "grad_norm": 2.836651563644409,
      "learning_rate": 1.8817226247576287e-05,
      "loss": 3.9763,
      "step": 1260
    },
    {
      "epoch": 0.1934206518428267,
      "grad_norm": 3.5323970317840576,
      "learning_rate": 1.8807021124604552e-05,
      "loss": 3.7843,
      "step": 1270
    },
    {
      "epoch": 0.1949436491014316,
      "grad_norm": 2.914527654647827,
      "learning_rate": 1.8796816001632822e-05,
      "loss": 3.863,
      "step": 1280
    },
    {
      "epoch": 0.19646664636003655,
      "grad_norm": 2.5619382858276367,
      "learning_rate": 1.8786610878661088e-05,
      "loss": 3.902,
      "step": 1290
    },
    {
      "epoch": 0.1979896436186415,
      "grad_norm": 2.9773941040039062,
      "learning_rate": 1.8776405755689357e-05,
      "loss": 3.8082,
      "step": 1300
    },
    {
      "epoch": 0.19951264087724643,
      "grad_norm": 2.960345506668091,
      "learning_rate": 1.8766200632717626e-05,
      "loss": 3.9064,
      "step": 1310
    },
    {
      "epoch": 0.20103563813585135,
      "grad_norm": 3.0530879497528076,
      "learning_rate": 1.8755995509745896e-05,
      "loss": 3.951,
      "step": 1320
    },
    {
      "epoch": 0.2025586353944563,
      "grad_norm": 2.749422788619995,
      "learning_rate": 1.874579038677416e-05,
      "loss": 3.8351,
      "step": 1330
    },
    {
      "epoch": 0.20408163265306123,
      "grad_norm": 2.9144904613494873,
      "learning_rate": 1.873558526380243e-05,
      "loss": 3.7106,
      "step": 1340
    },
    {
      "epoch": 0.20560462991166617,
      "grad_norm": 3.319197177886963,
      "learning_rate": 1.8725380140830697e-05,
      "loss": 3.9787,
      "step": 1350
    },
    {
      "epoch": 0.20712762717027108,
      "grad_norm": 2.6673877239227295,
      "learning_rate": 1.8715175017858966e-05,
      "loss": 3.8239,
      "step": 1360
    },
    {
      "epoch": 0.20865062442887602,
      "grad_norm": 3.176851987838745,
      "learning_rate": 1.8704969894887236e-05,
      "loss": 3.8894,
      "step": 1370
    },
    {
      "epoch": 0.21017362168748097,
      "grad_norm": 2.838872194290161,
      "learning_rate": 1.8694764771915505e-05,
      "loss": 3.7889,
      "step": 1380
    },
    {
      "epoch": 0.2116966189460859,
      "grad_norm": 2.678633689880371,
      "learning_rate": 1.868455964894377e-05,
      "loss": 3.8188,
      "step": 1390
    },
    {
      "epoch": 0.21321961620469082,
      "grad_norm": 2.6752779483795166,
      "learning_rate": 1.867435452597204e-05,
      "loss": 3.8182,
      "step": 1400
    },
    {
      "epoch": 0.21474261346329576,
      "grad_norm": 2.6798784732818604,
      "learning_rate": 1.8664149403000306e-05,
      "loss": 3.8923,
      "step": 1410
    },
    {
      "epoch": 0.2162656107219007,
      "grad_norm": 2.8388888835906982,
      "learning_rate": 1.8653944280028575e-05,
      "loss": 3.953,
      "step": 1420
    },
    {
      "epoch": 0.21778860798050564,
      "grad_norm": 2.8437135219573975,
      "learning_rate": 1.8643739157056845e-05,
      "loss": 3.8944,
      "step": 1430
    },
    {
      "epoch": 0.21931160523911056,
      "grad_norm": 2.859041929244995,
      "learning_rate": 1.863353403408511e-05,
      "loss": 3.8532,
      "step": 1440
    },
    {
      "epoch": 0.2208346024977155,
      "grad_norm": 2.6372387409210205,
      "learning_rate": 1.862332891111338e-05,
      "loss": 3.9119,
      "step": 1450
    },
    {
      "epoch": 0.22235759975632044,
      "grad_norm": 2.907119035720825,
      "learning_rate": 1.861312378814165e-05,
      "loss": 3.8551,
      "step": 1460
    },
    {
      "epoch": 0.22388059701492538,
      "grad_norm": 2.6918129920959473,
      "learning_rate": 1.860291866516992e-05,
      "loss": 3.9236,
      "step": 1470
    },
    {
      "epoch": 0.2254035942735303,
      "grad_norm": 2.399765729904175,
      "learning_rate": 1.8592713542198185e-05,
      "loss": 3.8672,
      "step": 1480
    },
    {
      "epoch": 0.22692659153213524,
      "grad_norm": 2.7191686630249023,
      "learning_rate": 1.8582508419226454e-05,
      "loss": 3.8579,
      "step": 1490
    },
    {
      "epoch": 0.22844958879074018,
      "grad_norm": 2.6084299087524414,
      "learning_rate": 1.857230329625472e-05,
      "loss": 3.875,
      "step": 1500
    }
  ],
  "logging_steps": 10,
  "max_steps": 19698,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1567752192000000.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
